---
title: "Why we need protocols in science"
author: 
  - Stefan Thoma
format:
   roche-revealjs: default
bibliography: references.bib
---

<!-- to publish:  -->

<!-- quarto publish gh-pages 2024_unibe_protocol.qmd -->

# About me

::: columns
::: {.column width="60%"}
-   MSc Psychology in Methods, Cognition & Perception at Unibe
-   MSc in Statistics at ETHZ
:::

::: {.column width="40%"}
![](images/useR_Presentation.JPG)
:::
:::

::: notes
Hello everyone,

Today, I want to talk about the importance of protocols and statistical analysis plans in clinical trials, and why regulators are so important in my opinion.

But first, I'll talk a bit about myself, the work that I do, about the replication crisis in psychology - and how this replication crisis relates to the importance of the protocol.

So, about myself: Originally, I'm from Baselland. But I studied Psychology in Bern and did masters in cognitive psychology and research method.

Then I did a master in statistics at ETH.

I currently live in Bern and Work at Roche Basel as a data scientist.
:::

## Current work

::: columns
::: {.column width="60%"}
-   At Roche working in a blended role as
    -   Statistical programmer
    -   Biostatistician
    -   R package developer, e.g. {admiral}
:::

::: {.column width="40%"}
![](images/Roche_Tower.jpg)
:::
:::

::: notes
I have been working at Roche for about 2 years. For anyone who doesn't know, Roche is a giant pharmaceutical company with around 100 000 employees. I work in Pharma Product Development in the Data and Statistical sciences team. We are around 300 Data scientists, data managers and statisticians sharing three floors in one of the towers in Basel. I don't know exactly how many people work in our team globally, but quite a few more.

The main goal of our team is to generate evidence of whether a drug is efficacious and safe for treating specific diseases. The way we do that is by designing and evaluating clinical trials in close collaboration with clinical scientists, physicians, and patients. If a drug works as we intend, we must provide valid and reliable evidence to the regulators to be able to get the drug on the market.

The data science aspect of this involves many different tasks, and at Roche this is typically split across a team of data scientists. On a clinical trial team you would typically have at least one Biostatistician, one statistical programmer, and a data manager.

I was originally hired as a statistical programmer where I would program standarised data-sets, run analyses, and generate outputs for the clinical study report. Incidentally, the report would then be written by medical writers, another role.

A biostatistician on the other hand would be more responsible for planning the trial, deciding on the right statistical analyses, and ensuring trial integrity from a statistical perspective.

As I have a degree in statistics, I am now also involved in some projects as a statistician. So now I am in what we call a fluid role as both a statistician and statistical programmer, and I am also involved in some r package development projects.
:::

## Replication crisis in Psychology

-   Researchers are unable to replicate many findings from psychological experiments.
    -   @klein2018 managed to replicate only 15 / 28 studies
    -   @opensciencecollaboration2015 replicated only 36 / 100 studies, with on average half of the original effect size
-   @yarkoni2022: How generalisable are findings from psychological experiments?

::: notes
Lets talk a bit about the replication crisis in psychology. This was quite transformative in my thinking about science and how we do research, and made me really appreciate the importance of regulations in the work I do now.

The replication crisis in psychology is a well-known phenomenon where researchers are unable to replicate many findings from psychological experiments. There are many studies highlighting this around 2015, and I want to show only two of them, the first and the second many-labs studies.

The many labs studies are large scale projects spanning multiple labs and countries. Authors essentially selected a number of studies from current and past literature and tried to replicate the findings.

There are different versions of this approach, in some of them the studies were replicated with input from the original authors, in others they were not. Generally, the replications are based on a much larger sample size, and they were conducted by various labs around the world. Findings differed but can largely be summarised that many studies could not be replicated.

Even under the exact same conditions, the original effect size was often not found. If it was found, the effect size was often much smaller than the original study.

This is a big problem, because if we can't replicate findings, its likely these findings don't exist at all. If we cannot trust these findings, why would we invest into them as a society?

As an aside, Tal Yarkoni goes even further with the question of how generalisable findings from psychological experiments are, even if they were replicable. The paper I linked here is a really great read if you want to dive deeper, but let's move on for now.
:::

## Many labs result

![Results of @opensciencecollaboration2015](images/349_aac4716_fa.jpeg)

::: notes
This is a figure from the many labs study. It plots on the x axis the effect size of the original study. On the y axis, it we see the effect size of the replication study.

The green color indicates that the replication study found a significant effect, the red color indicates that the replication study did not find a significant effect. If the replication study found the same effect size as the original study, the point would be on the diagonal line.

In allmost all of the cases in this many labs study the replication effect sizes were smaller than the original effect sizes. Sometimes they would even go into the opposite direction. We can see here also that most studies were not replicated.

So, how did we get into this situation?
:::

## Misaligned incentives for researchers

-   Publication bias

    -   Harder and less likely to publish null-results

    -   More likely to continue looking for something

-   Questionable research practices undermine trial integrity

    -   Hypothesising After Results are Known (HARK-ing)

    -   P-Hacking

-   Fraud, for an example see Data Colada posts by @simonsohn2023

    -   Hard to spot, as open data was not very common

-   Doing replications is not "novel research", less prestigious

::: notes
There are many reasons why we are in this situation, and I want to highlight some of them.

I think the main reason for the replication crisis is that the incentives for researchers are misaligned.

Let's start with the publication bias, which states that studies that don't find significant results are less likely to be published. This is because journals are more likely to publish novel and exciting results. On its face, that may seem reasonable. However, if you design a study to test a hypothesis, you are trying to broaden the knowledge in a specific area. And knowing that a hypothesis is not true is also a very valuable piece of information and would contribute to the field as well.

Knowing that journals might not want to publish a null result, will also make the researcher to simply skip the publication process and move on to the next hypothesis.

Questionable research practices are behaviours of reserachers that tie into this search for positive results. They are very often not done on purpose, and are often a result of the pressure to publish and a lack of statistical training.

P-hacking is one of these practices, where researchers try out different statistical tests until they find a significant result. This, of course, is not a valid way to go about. The method used should be determined before data analysis.

The same goes for hypothesising after the results are known, or HARKing. That would mean setting up a trial, looking at data, and then readjusting your hypothesis to get significant results, only to then publish the results as if that was your hypothesis from the start. This is essentially passing exploratory research as confirmatory research, and heavily inflates the probability of false positive results.

Here I would like to emphasize the the easiest person to fool is yourself. Once you have looked at the data and tried out some analyses you will have a hard time remembering what your initial plan was, if you did not write it down beforehand to some extent.

Fraud is also a problem, but it is not as common as the other practices. However, it is very hard to spot, especially if the data is not openly available. This is why open data is so important, and why I am a big proponent of it. I link here a very enlightening blogpost discussing a prominent fraud case by a famous psychologist known for his honesty research.

Lastly, doing replications is not considered novel research, and is therefore less prestigious. This is a problem, because replications are a very important part of the scientific process. Especially so, when the original study results are very surprising or counterintuitive, the effect size was large, and the sample size was small.
:::

## Realign researcher incentives

**Move away from rewarding the most headline-grabbing findings**

-   Preregistration, see [OSF](https://osf.io/)
    -   Support trial integrity / validity
    -   Registered reports almost guarantee publication before data collection @chambers2019
    -   Published based on the merits of the hypothesis and methods, independent of results

![From @science](images/registered_reports.width-800.png)

-   Open research
    -   Openly available data and methods
        -   Encourage collaboration
        -   Reduce ease of fraud
    -   Openly available publications
-   Data as publication rewards data collection
    -   Increases data quality
    -   Treated as legitimate research

::: notes
So, how can we realign the incentives for researchers?

One way is to move away from rewarding the most headline-grabbing findings.

One way to do this is to preregister your study. Preregistration is the practice of writing down your research plan before you start collecting data. This generally includes your research question, your hypothesis, your methods, and your analysis plan. Ideally, this should be done before even starting the experiment, and it is a good idea to publish this plan so that you cannot change it afterwards. One place where you can do this is the Open Science Framework, which is a platform for open research practices. They also provide really good templates for preregistrations.

A step further are preregistered reports. These are papers that are basically already written and approved for publication by a journal before the data is collected. This is a great way to ensure that the publication is based on the merits of the hypothesis and methods, independent of the results.

Other structural changes may benefit the field as well. Open research is another way to realign incentives. This means that data and methods are openly available. This encourages collaboration, and reduces the ease of fraud. It also makes it easier to replicate studies, and to build on previous research.

Data as publication is a way to reward data collection. This increases data quality, and treats data collection as legitimate research. In a way, this allows researchers to focus on separate aspects of research, and rewards not just the study outcomes but also the way to get there, allowing also for different paths to researchers success.
:::

## Things are changing

Structural changes are happening!

![From @korbmacher2023](images/44271_2023_3_Fig1_HTML.webp)

::: notes
I want to end this on a positive note, and say that things are changing. There are structural changes happening, and the field is moving towards a more open and transparent way of doing research. Some journals follow suit by allowing for registered reports, and open data and methods. @centerforopenscience2023

However, it is important to remember that these changes are not yet the norm, and that it is up to us as researchers to push for these changes. So on an institutional level these positive practices are not mandated.
:::

<!-- | Many psychological effects experiments have failed \| -->

<!-- | maybe talk about baserate problem. e.g. if you have 1000 hypothesis and they have 10% chance of being true, then you would find 80 studies to be truly positive (80% power), but would 900 \* alpha% = 45 be falsely true out of 100 studies @klein2018 \|-->

## Protocol - Study blueprint

-   Includes:
    -   Motivation and objectives
    -   Primary and key secondary endpoints, as estimands
    -   Methodology
    -   Organisation
    -   Some statistical considerations, e.g. sample size & power, etc.
-   Details should allow recreation of the study

::: notes
When I switched to pharma, I was very happy to learn that practices similar to those described above were not just standard in the industry, but mandated and followed up on by regulators.

Protocols are required and an important part of setting up a clinical trial.

Th Protocol is a study blueprint that includes the motivation and objectives of the study, the primary and key secondary endpoints as estimands, the methodology, the organisation, and some statistical considerations, such as sample size and power.

Protocols should be detailed enough to allow other researchers to recreate the study. With a proper protocol in place, you should be unable to pass off exploratory research as confirmatory research. Of course, the protocol can be changed, but this should be done in a transparent way, and the changes must be justified, else you get in trouble with the regulators.
:::

## Estimands

An **estimand** (@pohl2021) is a precise description of the treatment effect to be estimated in [a clinical trial](https://forpatients.roche.com/en/trials/muscle-and-peripheral-nerve-disease/dmd/an-open-label-study-to-assess-the-efficacy-and-safety-o-12382.html). - "What exactly do we want to learn from the trial?"

-   **Key Attributes:**
    -   **Population:** The specific group of patients of interest.
    -   **Treatment:** The intervention or comparison of interest.
    -   **Outcome:** The specific outcome variable being measured.
    -   **Intercurrent Events:** Events that occur after treatment initiation that affect the interpretation of the outcome (e.g., use of rescue medication, treatment discontinuation).
    -   **Population-level Summary:** The summary measure that will be used to quantify the treatment effect (e.g., difference in means, hazard ratio).

::: notes
I mentioned estimands before when talking about endpoints. I want to explain this a bit more, and I'll use one of my current trials as an example, although I simplify the example a little.

We use estimands in our protocols to define exactly what question we want to answer with our study. We can have multiple estimands, for multiple studies, their importance is usually hierarchical. These estimands force us to think about the five most important aspects of our hypothesis.

Namely, the population, ie. to whom does the hypothesis apply? To which people is the estimated effect relevant and generalizable? In [one study](https://forpatients.roche.com/en/trials/muscle-and-peripheral-nerve-disease/dmd/an-open-label-study-to-assess-the-efficacy-and-safety-o-12382.html) I currently work on the population for the primary Estimand could be defined as "ambulatory and non-ambulatory patients with Duchenne muscular dystrophy (DMD) age ≥ 8 to \< 16 years old receiving corticosteroid therapy"

The treatment, what is the intervention or comparison of interest? Again, in the study I work on, the treatment is the investigational drug, given as an injection under the skin in the belly or thigh at Weeks 0, 2, 4, and every 4 weeks thereafter for 104 weeks. My study is a bit special as there is no control arm in the study, but the treatment is compared to real world data.

The outcome, what is the specific outcome variable being measured? For us that would be the change in Bone Mineral Density at week 52 compared to baseline.

Intercurrent events, what events could happen that would affect the interpretation of the outcome? In our study, we have a few intercurrent events, such as dropout from the study for reasons other than death, or the use of rescue medication. Such events may introduce missing data into our study, and we want to decide beforehand, how each specific case should be treated.

And finally, the population-level summary, what is the summary measure that will be used to quantify the treatment effect? This could be the mean difference of the outcome variable between the experimental group. In our special case it could be the mean difference of the Bone Mineral Density between the experimental group and the real-world data.
:::

## Statistical analysis plan

-   Describes the precise statistical analysis for each estimand.
-   Written before looking at the data
-   Confirmatory analysis with less researchers degree of freedom
-   Exploratory analysis still encouraged, but must be declared as such

::: notes
The statistical analysis plan is a document that describes the precise statistical analysis for each estimand. It is written before looking at the data, and it is important to stick to the plan.\
This is to avoid the so-called p-hacking, where researchers keep trying different analyses until they find one that is significant. It is also crucial to reduce the risk of false positives, and to increase the credibility of the results.

Very important to note here is that sticking to the SAP does not mean that data exploration is no longer possible, but it means that the confirmatory analysis is pre-specified, and that the exploratory analysis is declared as such.
:::

## Regulation

-   Encourages good scientific practice
-   Ensures transparency and reproducibility
-   Encourages good scientific practice
-   Sets standards in data quality and documentation
-   Hurdle that every project needs to pass

::: notes
Regulation is a big part of the pharma industry, and it is a good thing. It encourages good scientific practice, ensures transparency and reproducibility, and sets standards in data quality and documentation. It is a hurdle that every project needs to pass, but it is a necessary one, to ensure that the results are reliable and trustworthy.

And regulators, especially the FDA and EMA, are being taken VERY seriously. On project teams, when faced with difficult decisions, we would often ask ourselves "What would the FDA say?"? Would they accept this? Even when the team would be convinced that a certain decision was the right one, if the FDA would not accept it, we would not do it. And in hindsight those were the right decisions.

In a way, in our minds, these regulators serve as a kind of objective scientist. Trying to put ourselves in their shoes helped us step back and look at the study with a more objective eye.
:::

## Summary

-   Protocols or Preregistrations ensure study integrity
-   Estimands define the question to be answered
    -   Take a closer look at Estimands for your own project
-   Statistical Analysis Plans ensure validity of confirmatory results
-   Regulators ensure good scientific practice

## Resources
